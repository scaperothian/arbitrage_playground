{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b5742c-d5c7-4623-82af-71b9e653c469",
   "metadata": {},
   "source": [
    "# LGBM and Percent Change\n",
    "\n",
    "Modeling to predict the percent change between the two pools of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zY2TavXk8y6g",
   "metadata": {
    "id": "zY2TavXk8y6g"
   },
   "source": [
    "# When Using a Google Shared Drive\n",
    "\n",
    "To mount a drive that is not yours but shared with you, you must:<br>\n",
    "- go into Google Drive<br>\n",
    "- select the DATASCI210 folder you want to use in Colab<br>\n",
    "- Right click on the DATASCI210 folder<br>\n",
    "- Select \"Organize\"<br>\n",
    "- then \"Add Shortcut\"<br>\n",
    "- then specify the shortcut to go into the \"MyDrive\" folder.<br>\n",
    "- Then this code will 'work' out of the box. What could go wrong? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e932f9-8482-4d23-b1e9-61c1fb2db8c6",
   "metadata": {
    "executionInfo": {
     "elapsed": 2115,
     "status": "ok",
     "timestamp": 1719599278353,
     "user": {
      "displayName": "John Forlines",
      "userId": "00296519355576197408"
     },
     "user_tz": 420
    },
    "id": "09e932f9-8482-4d23-b1e9-61c1fb2db8c6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "PdTe7tBp8iRf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23396,
     "status": "ok",
     "timestamp": 1719599301747,
     "user": {
      "displayName": "John Forlines",
      "userId": "00296519355576197408"
     },
     "user_tz": 420
    },
    "id": "PdTe7tBp8iRf",
    "outputId": "e268c679-ce27-42a3-901a-01777302a8bb"
   },
   "outputs": [],
   "source": [
    "GOOGLE_COLAB = False\n",
    "\n",
    "if GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  %cd /content/drive/MyDrive/DATASCI210/arbitrage_3M\n",
    "  print(os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71dc24ec-da0b-4bb6-9ae9-d6aced39ae64",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1719599301747,
     "user": {
      "displayName": "John Forlines",
      "userId": "00296519355576197408"
     },
     "user_tz": 420
    },
    "id": "71dc24ec-da0b-4bb6-9ae9-d6aced39ae64"
   },
   "outputs": [],
   "source": [
    "def extract_df(location):\n",
    "  \"\"\"\n",
    "  Funtion to extract transaction level files from Folder, iterates through all files looking for combination pool_id_XXXXXX\n",
    "\n",
    "  Location Updated to work in Google Drive.\n",
    "\n",
    "  \"\"\"\n",
    "  final_df = pd.DataFrame()\n",
    "\n",
    "  for i in [x for x in os.listdir(location) if x.find(f'arbitrage_20240313_20240613_WETH_USDC')!=-1]:\n",
    "    print(f\"Reading: {i}\")\n",
    "    temp_df = pd.read_csv(f\"{location}/{i}\")\n",
    "    try:\n",
    "      temp_df['time']\n",
    "      pass\n",
    "    except:\n",
    "      temp_df['DATE'] = temp_df['timestamp'].apply(lambda x: datetime.datetime.fromtimestamp(int(x)).replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "    final_df = pd.concat([final_df,temp_df])\n",
    "    #print('.',end='')\n",
    "\n",
    "  try:\n",
    "    final_df['DATE'] = final_df['time'].apply(lambda x: datetime.datetime(int(x[:4]),int(x[5:7]),int(x[8:10])))\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    return final_df.drop('Unnamed: 0',axis=1).reset_index(drop=True)\n",
    "  except:\n",
    "    return final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d05cdec-27a9-4adc-94e8-98118c90e0f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33018,
     "status": "ok",
     "timestamp": 1719599334763,
     "user": {
      "displayName": "John Forlines",
      "userId": "00296519355576197408"
     },
     "user_tz": 420
    },
    "id": "2d05cdec-27a9-4adc-94e8-98118c90e0f0",
    "outputId": "fd1a3b8f-bf56-4ac3-f863-47d06355718e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: arbitrage_20240313_20240613_WETH_USDC_9.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_8.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_15.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_14.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_16.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_17.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_13.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_12.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_10.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_11.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_20.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7k/rctwsj8n1z92y6cwvtwzc2y80000gn/T/ipykernel_39530/3355236209.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat([final_df,temp_df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: arbitrage_20240313_20240613_WETH_USDC_18.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_5.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_4.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_6.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_7.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_3.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_2.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_0.csv\n",
      "Reading: arbitrage_20240313_20240613_WETH_USDC_1.csv\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_PATH = \"../../arbitrage_3M/\"\n",
    "arbitrage_df = extract_df(TRAINING_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b7f900-50a8-4d8f-adae-c69037570a09",
   "metadata": {
    "id": "e3b7f900-50a8-4d8f-adae-c69037570a09"
   },
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "convert time to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd59136-8f08-4c85-91a3-f5efaf853773",
   "metadata": {
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1719599336855,
     "user": {
      "displayName": "John Forlines",
      "userId": "00296519355576197408"
     },
     "user_tz": 420
    },
    "id": "9bd59136-8f08-4c85-91a3-f5efaf853773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaNs in the DataFrame? False\n"
     ]
    }
   ],
   "source": [
    "arbitrage_sorted_df = arbitrage_df.sort_values(by='time', ascending=True)\n",
    "arbitrage_sorted_df['time'] = pd.to_datetime(arbitrage_sorted_df['time'], format='ISO8601')\n",
    "arbitrage_sorted_df = arbitrage_sorted_df.reset_index(drop=True)\n",
    "\n",
    "# Find the first row with NaNs...\n",
    "new_first_row = arbitrage_sorted_df['p0.eth_price_usd'].first_valid_index()\n",
    "arbitrage_sorted_df = arbitrage_sorted_df.iloc[new_first_row:]\n",
    "# remove NaNs from forward fill\n",
    "has_nans = arbitrage_sorted_df.isna().any().any()\n",
    "print(\"Are there any NaNs in the DataFrame?\", has_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d728a987-7b5e-4499-89fb-407ede4fba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Time (in seconds) between transactions: 11.78\n",
      "Transactions per minute: 3.66\n"
     ]
    }
   ],
   "source": [
    "# Here are some transaction rates from another notebook: \n",
    "# Pool 0 (0.3% fee): 8818.74hr of transactions or 11.7798 tph.\n",
    "# Pool 1 (0.05% fee): 8819.09hr of transactions or 219.4892 tph.\n",
    "rate0 = 11.7798\n",
    "rate1 = 219.4892\n",
    "\n",
    "# What is the mean time between transactions in the dataset: \n",
    "ds_seconds = arbitrage_sorted_df['time'].diff().iloc[1:].dt.total_seconds()\n",
    "print(f\"Mean Time (in seconds) between transactions: {np.array(ds_seconds).mean():.2f}\")\n",
    "print(f\"Transactions per minute: {rate1 / 60:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd51c346-0b0c-4a24-879c-a8725fb1b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions in Pool 0 (0.3%  fee) over three months: 33081\n",
      "Transactions in Pool 1 (0.05% fee) over three months: 380653\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transactions in Pool 0 (0.3%  fee) over three months: {arbitrage_sorted_df['p0.transaction_time'].unique().shape[0]}\")\n",
    "print(f\"Transactions in Pool 1 (0.05% fee) over three months: {arbitrage_sorted_df['p1.transaction_time'].unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dbc828c3-4a0b-477f-9ab5-8f35a8e9ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_timestamp(df, time_col, minutes):\n",
    "    # Ensure 'time' column is in datetime format\n",
    "    df.loc[:, time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # Create a shifted version of the DataFrame with the target times\n",
    "    shifted_df = df.copy()\n",
    "    shifted_df[time_col] = shifted_df[time_col] - pd.Timedelta(minutes=minutes)\n",
    "\n",
    "    # Merge the original DataFrame with the shifted DataFrame on the closest timestamps\n",
    "    result_df = pd.merge_asof(df.sort_values(by=time_col),\n",
    "                              shifted_df.sort_values(by=time_col),\n",
    "                              on=time_col,\n",
    "                              direction='backward',\n",
    "                              suffixes=('', '_label'))\n",
    "\n",
    "    # Select the required columns and rename them\n",
    "    result_df = result_df[['time','percent_change','percent_change_label']]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891043a-88c6-49e3-b31e-7394fb48eb49",
   "metadata": {},
   "source": [
    "# LGBM Training \n",
    "1 Minute Intervals - not fixed, 2 Lags, 8 tap rolling mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ba323-3277-42a5-a24a-dc9ff7164a39",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cd618140-a086-44da-bcd2-8a8535474a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_WINDOW_MIN=1\n",
    "MODEL_NAME=\"LGBM\"\n",
    "\n",
    "# model parameters (things that can be ablated using the same data)\n",
    "NUM_LAGS = 2  # Number of lags to create\n",
    "N_WINDOW_AVERAGE = 8 # rollling mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "63706145-a1c2-4bf7-8eaa-132fecbc2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaNs in the DataFrame? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = []\n",
    "\n",
    "int_df = arbitrage_sorted_df[['time','percent_change']]\n",
    "int_df = find_closest_timestamp(int_df, 'time', FORECAST_WINDOW_MIN)\n",
    "\n",
    "for i in range(1, NUM_LAGS + 1):\n",
    "    int_df[f'lag_{i}'] = int_df['percent_change'].shift(i)\n",
    "\n",
    "int_df[f'rolling_mean_{N_WINDOW_AVERAGE}'] = int_df['percent_change'].rolling(window=N_WINDOW_AVERAGE).mean()\n",
    "\n",
    "# prune excess rows from lagging operation\n",
    "max_prune = max(N_WINDOW_AVERAGE,NUM_LAGS)\n",
    "int_df = int_df.iloc[max_prune-1:]\n",
    "\n",
    "has_nans = int_df.isna().any().any()\n",
    "print(\"Are there any NaNs in the DataFrame?\", has_nans)\n",
    "if has_nans: \n",
    "    print(f\"Found: {int_df.isna().sum()}\")\n",
    "\n",
    "# Create time index foer the dataframe.\n",
    "int_df.index = int_df.pop('time')\n",
    "int_df.index = pd.to_datetime(int_df.index)\n",
    "\n",
    "# Create labels and training...\n",
    "y = int_df.pop('percent_change_label')\n",
    "X = int_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "808ac187-ad0e-41d3-8c8c-b74bb4fa8e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['percent_change', 'lag_1', 'lag_2', 'rolling_mean_8'], dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns used for inference:\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c3fa3d97-befe-4041-a51d-7dac7582eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 541286, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000203\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttrain's l2: 8.12011e-07\tvalid's l2: 3.17142e-07\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=False)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10, verbose=True),\n",
    "        lgb.record_evaluation(evals_result)\n",
    "        ]\n",
    ")\n",
    "\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "metrics.append({\n",
    "    'MSE':mse,\n",
    "    'RMSE':np.sqrt(mse),\n",
    "    'R Squared':r2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e6730-214a-4b4a-8f42-993f1ee4494e",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "489580a5-f8a7-4a85-a599-4cb83f3b57c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0005631538850498971\n",
      "R Squared: 0.8803237684440066\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE:\", np.sqrt(mse))\n",
    "print(\"R Squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "760bdb1b-fda4-44ea-afbe-8d94c25a9a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any NaNs in the DataFrame? False\n"
     ]
    }
   ],
   "source": [
    "df_5min = CalculateInterval(arbitrage_sorted_df,\n",
    "                      'percent_change',\n",
    "                      'time',\n",
    "                      5)\n",
    "\n",
    "has_nans = df_5min.isna().any().any()\n",
    "print(\"Are there any NaNs in the DataFrame?\", has_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fc43261c-7b1e-4d70-b9eb-08233c320ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 21260, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score -0.000081\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttrain's l2: 9.51062e-07\tvalid's l2: 4.84189e-07\n"
     ]
    }
   ],
   "source": [
    "NLAGS = 3\n",
    "metrics = []\n",
    "\n",
    "df_dataset = df_5min.copy()[['time','mean']].sort_values(by='time',ascending=True).reset_index(drop=True)\n",
    "df_dataset.columns=['time','percent_change']\n",
    "for lag in range(1,NLAGS):\n",
    "    df_dataset[f'percent_change_lag{lag}'] = df_dataset['percent_change'].shift(lag)\n",
    "\n",
    "# Remove the last row because of the lags\n",
    "df_dataset = df_dataset.iloc[NLAGS-1:]\n",
    "# Create time index foer the dataframe.\n",
    "df_dataset.index = df_dataset.pop('time')\n",
    "df_dataset.index = pd.to_datetime(df_dataset.index)\n",
    "\n",
    "X = df_dataset.copy().iloc[:-1]\n",
    "y = df_dataset['percent_change'].shift(-1).iloc[:-1] # shift by 1 to get next valid 1 min prediction labels (dependson the moving average)\n",
    "\n",
    "if df_dataset.isna().any().any():\n",
    "    print(\"NaNs in the dataset prior to the training\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=False)\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.5,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5\n",
    "}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "gbm = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10, verbose=True),\n",
    "        lgb.record_evaluation(evals_result)\n",
    "        ]\n",
    ")\n",
    "\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "metrics.append({\n",
    "    'MSE':mse,\n",
    "    'RMSE':np.sqrt(mse),\n",
    "    'R Squared':r2\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074758f-462d-4052-87f3-3db482dc2cd5",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8cd9113e-5460-48b0-b740-9b3c7a12de1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 106309, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000082\n",
      "RMSE: 0.0004185312127370536\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 106309, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000082\n",
      "RMSE: 0.0002362610072655536\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 106309, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000082\n",
      "RMSE: 0.0003648671578966606\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 106309, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000082\n",
      "RMSE: 0.00040771339199358216\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000417 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 106309, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000082\n",
      "RMSE: 0.0004189680617572804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 106309, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score -0.000082\n",
      "RMSE: 0.0004182915136309256\n"
     ]
    }
   ],
   "source": [
    "SPANS=[1,3,10,30,100,300]\n",
    "NLAGS = 3\n",
    "metrics = []\n",
    "for SPAN in SPANS: \n",
    "\n",
    "    #\n",
    "    # NOTE: This is different from the other cells.  To get the ewm method to work where its taking the \n",
    "    #       most recent timestamps and averaging backward only, you must sort descending.\n",
    "    #       THEN you must return the sort to ascending to be consistent with the LAGS and LABELING done\n",
    "    #       earlier.\n",
    "    #\n",
    "    df_dataset = df_1min.copy()[['time','mean']].sort_values(by='time',ascending=False).reset_index(drop=True)\n",
    "    df_dataset.columns=['time','percent_change']\n",
    "    df_dataset['percent_change_ewm'] = df_dataset['percent_change'].ewm(span=SPAN, adjust=False).mean()\n",
    "    \n",
    "    #\n",
    "    # Returning order to ascending to get the lags back in the direction required.\n",
    "    #\n",
    "    df_dataset = df_dataset.sort_values(by='time',ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    for lag in range(1,NLAGS):\n",
    "        df_dataset[f'percent_change_lag{lag}'] = df_dataset['percent_change'].shift(lag)\n",
    "    # Remove the last row because of the lags\n",
    "    df_dataset = df_dataset.iloc[NLAGS-1:]\n",
    "    \n",
    "    # Create time index foer the dataframe.\n",
    "    df_dataset.index = df_dataset.pop('time')\n",
    "    df_dataset.index = pd.to_datetime(df_dataset.index)\n",
    "\n",
    "    \n",
    "    # Because this is next step 1 min prediction, we need to crop inputs and shift and crop labels\n",
    "    y = df_dataset['percent_change'].shift(-1).iloc[:-1]\n",
    "    X = df_dataset.copy().iloc[:-1]\n",
    "    \n",
    "    if df_dataset.isna().any().any():\n",
    "        print(\"NaNs in the dataset prior to the training\")\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,shuffle=False)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test)\n",
    "    \n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.5,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5\n",
    "    }\n",
    "    \n",
    "    evals_result = {}\n",
    "    \n",
    "    \n",
    "    gbm = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=[train_data, test_data],\n",
    "        valid_names=['train', 'valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=10, verbose=False),\n",
    "            lgb.record_evaluation(evals_result)\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    metrics.append({\n",
    "        'Exponential Moving Average (1 min intervals)':SPAN,\n",
    "        'MSE':mse,\n",
    "        'RMSE':np.sqrt(mse),\n",
    "        'R Squared':r2\n",
    "    })\n",
    "    print(\"RMSE:\", np.sqrt(mse))\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "36149cdf-2670-47ca-8227-74a20cff8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbm.save_model(f'percent_change_{FORECAST_WINDOW_MIN}min_forecast_{MODEL_NAME}.pkl')\n",
    "# Save the model with pickle\n",
    "with open(f'percent_change_{FORECAST_WINDOW_MIN}min_forecast_{MODEL_NAME}.pkl', 'wb') as f:\n",
    "    pickle.dump(gbm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b9ea6-b045-4445-9131-dcb80776245d",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e75aa71f-17fb-4d32-8f9c-4756720229ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_model(model_name):\n",
    "    models_dir = os.path.join(os.getcwd(), '.')\n",
    "    base_model_path = os.path.join(models_dir, model_name)\n",
    "    \n",
    "    # Check for different possible file extensions\n",
    "    possible_extensions = ['', '.h5', '.pkl', '.joblib']\n",
    "    model_path = next((base_model_path + ext for ext in possible_extensions if os.path.exists(base_model_path + ext)), None)\n",
    "    if model_path is None:\n",
    "        print(f\"Model file not found for: {model_name}\")\n",
    "        return None\n",
    "    print(model_path)\n",
    "    try:\n",
    "        #if model_name.startswith(\"LSTM\"):\n",
    "        #    model = tf.keras.models.load_model(model_path)\n",
    "        #else:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model {model_name} loaded successfully from {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2811d563-e235-4c11-a5e7-3435d4585891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 minute forecast\n"
     ]
    }
   ],
   "source": [
    "print(f\"{FORECAST_WINDOW_MIN} minute forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f1b7a265-038b-4548-ab32-42a5599f21f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.000945365210858"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a2cebff1-8bc9-4431-9ce1-a18cfe653513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percent_change        -0.000946\n",
       "percent_change_ewm    -0.000536\n",
       "percent_change_lag1   -0.001001\n",
       "percent_change_lag2   -0.001189\n",
       "Name: 2024-05-25 20:07:00, dtype: float64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d163c284-7e3a-4021-ac68-a5a5fb78ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/das/DATASCI210/arbitrage_playground/notebooks/./percent_change_1min_forecast_LGBM.pkl\n",
      "Model percent_change_1min_forecast_LGBM loaded successfully from /Users/das/DATASCI210/arbitrage_playground/notebooks/./percent_change_1min_forecast_LGBM.pkl\n",
      "-0.0009436166232362283\n"
     ]
    }
   ],
   "source": [
    "#loaded_model = lgb.Booster(model_file=f'percent_change_{FORECAST_WINDOW_MIN}min_forecast_{MODEL_NAME}.pkl')\n",
    "loaded_model = load_model(f'percent_change_{FORECAST_WINDOW_MIN}min_forecast_{MODEL_NAME}')\n",
    "if loaded_model is None: \n",
    "    print('No model found.')\n",
    "else:\n",
    "    # make sure that pulling a single sample still results\n",
    "    # in a properly formated data frame. 1 bracket gives\n",
    "    # you a DataSeries, 2 gives you a Dataframe.\n",
    "    print(loaded_model.predict(X_test.iloc[[0]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d4a2f-0fd1-49d2-bdd4-bf72d867239e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1T_Xp9xUgviP7Bch4SMxS2xnXkqvdDaE9",
     "timestamp": 1719546171379
    },
    {
     "file_id": "14v23Z5h8zLP9YBxoMs5NABdfW_TRVJsY",
     "timestamp": 1719545876782
    },
    {
     "file_id": "1oHlpBHg2NNOBd6fewtv2aGj9715Q-Pt1",
     "timestamp": 1719502116674
    }
   ]
  },
  "kernelspec": {
   "display_name": "ds210v2 (with Poetry)",
   "language": "python",
   "name": "ds210v2-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
